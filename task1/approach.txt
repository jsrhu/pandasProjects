Now that you have the data, the first task is to verify all their fields, e,g, if source_rank is claimed to be from [1,10], then this should be true every single day.
To do this, what you have to do is to plot the daily maxes and daily mins for source_rank for each day (see plot below for an example).
Note that python has a lot of vectorized operations.
If you find that you're doing a brute force iteration, i.e.

	for i in range( N ):
		do stuff on array[i]

then there may be a faster way.
Avoid brute force iteration if at all possible.

This verification can be done for any bounded numeric field.
And we should certainly verify all of them.

Secondly, we can also aggregate a list of URLs which they scrape.
I think it would be interested in compiling a catalog of unique URLs.
You would essentially extract the URL pandas series from the dataframe look at the source website (this will involve using split) and then keep unique values.

The same thing could be done for the list of tickers.
-----------------------------------

APPROACH: Value Range Confirmation

1) rbbst (red-black binary search tree):
	Performance:
		Space:
			Average:
				O(n)
			Worst:
				O(n)

		Search:
			Average:
				O(log n)
			Worst:
				O(log n)

		Insert:
			Average:
				O(log n)
			Worst:
				O(log n)

		Delete:
			Average:
				O(log n)
			Worst:
				O(log n

	Reasoning:
		Simple construction
			- insert value and sink into tree
		Simple confirmation of range
			- one algortihm each to find max and min values of a tree

	Satisfies:
		Confirmation of quantitative fields

	Does Not Satisfy:
		Confirmation of qualitative fields

	NOTE:
		Does not guarantee confirmation on (virtual) max and min values of data field, only if daily max and min is within claimed range

APPROACH: Aggregate URL List

1)
